{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:56:15.828267Z",
     "start_time": "2020-07-25T00:56:15.563057Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "np.random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:56:16.098058Z",
     "start_time": "2020-07-25T00:56:16.071109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files:  2700\n",
      "Showing first 10 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./ExtractedFeatures/csv/2/2_450.csv',\n",
       " './ExtractedFeatures/csv/2/2_1864.csv',\n",
       " './ExtractedFeatures/csv/2/2_336.csv',\n",
       " './ExtractedFeatures/csv/2/2_322.csv',\n",
       " './ExtractedFeatures/csv/2/2_1870.csv',\n",
       " './ExtractedFeatures/csv/2/2_444.csv',\n",
       " './ExtractedFeatures/csv/2/2_2389.csv',\n",
       " './ExtractedFeatures/csv/2/2_1680.csv',\n",
       " './ExtractedFeatures/csv/2/2_1858.csv',\n",
       " './ExtractedFeatures/csv/2/2_1694.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files = glob.glob(\"./ExtractedFeatures/csv/*\")\n",
    "files = glob.glob(\"./ExtractedFeatures/csv/2/*\")\n",
    "print(\"Total number of files: \", len(files))\n",
    "print(\"Showing first 10 files...\")\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:56:18.968441Z",
     "start_time": "2020-07-25T00:56:18.564411Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:56:36.109391Z",
     "start_time": "2020-07-25T00:56:34.286877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:08.071733Z",
     "start_time": "2020-07-25T00:58:08.063303Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def tf_data_generator(file_list, batch_size = 20):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i*batch_size >= len(file_list):  \n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)\n",
    "        else:\n",
    "            file_chunk = file_list[i*batch_size:(i+1)*batch_size] \n",
    "            data = []\n",
    "            labels = []\n",
    "            label_classes = tf.constant([\"0\", \"1\", \"2\"])\n",
    "            for file in file_chunk:\n",
    "                #temp = np.genfromtxt(file, max_rows=4000, dtype=float)\n",
    "                temp = pd.read_csv(open(file,'r'), sep='\\s+',skiprows=0,nrows=4500,usecols = [0,1,2,3,4]) # Change this lfault_foldersne to read any other type of file\n",
    "                temp = temp.dropna()\n",
    "                #print(temp.shape)\n",
    "                #print(temp[:10])\n",
    "                temp = temp[:4000]\n",
    "                #temp = np.squeeze(np.asarray(temp))\n",
    "                data.append(temp.values.reshape(4000,5,1)) # Convert column data to matrix like data with one channel\n",
    "                pattern = tf.constant(eval(\"file[24:25]\"))      # Pattern extracted from file_name\n",
    "                for j in range(len(label_classes)):\n",
    "                    if re.match(pattern.numpy(), label_classes[j].numpy()): \n",
    "                        labels.append(j)  \n",
    "            data = np.asarray(data).reshape(-1,4000,5,1)\n",
    "            labels = np.asarray(labels)\n",
    "            yield data, labels\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:08.835905Z",
     "start_time": "2020-07-25T00:58:08.832867Z"
    }
   },
   "outputs": [],
   "source": [
    "check_data = tf_data_generator(files, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:10.144423Z",
     "start_time": "2020-07-25T00:58:09.293159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4000, 5, 1) (10,)\n",
      "[2 2 2 2 2 2 2 2 2 2] <--Labels\n",
      "\n",
      "(10, 4000, 5, 1) (10,)\n",
      "[2 2 2 2 2 2 2 2 2 2] <--Labels\n",
      "\n",
      "(10, 4000, 5, 1) (10,)\n",
      "[2 2 2 2 2 2 2 2 2 2] <--Labels\n",
      "\n",
      "(10, 4000, 5, 1) (10,)\n",
      "[2 2 2 2 2 2 2 2 2 2] <--Labels\n",
      "\n",
      "(10, 4000, 5, 1) (10,)\n",
      "[2 2 2 2 2 2 2 2 2 2] <--Labels\n",
      "\n",
      "(10, 4000, 5, 1) (10,)\n",
      "[2 2 2 2 2 2 2 2 2 2] <--Labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for data, labels in check_data:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels, \"<--Labels\")\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:14.282621Z",
     "start_time": "2020-07-25T00:58:14.278169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['-0.16']\n",
      "   ['0.38']\n",
      "   ['0.07']\n",
      "   [0.24]\n",
      "   [-0.18]]\n",
      "\n",
      "  [['-0.16']\n",
      "   ['0.38']\n",
      "   ['0.07']\n",
      "   [0.24]\n",
      "   [-0.18]]\n",
      "\n",
      "  [['-0.15']\n",
      "   ['0.38']\n",
      "   ['0.07']\n",
      "   [0.24]\n",
      "   [-0.18]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [['0.94']\n",
      "   ['-0.09']\n",
      "   ['0.23']\n",
      "   [0.15]\n",
      "   [0.12]]\n",
      "\n",
      "  [['0.93']\n",
      "   ['-0.09']\n",
      "   ['0.23']\n",
      "   [0.15]\n",
      "   [0.12]]\n",
      "\n",
      "  [['0.92']\n",
      "   ['-0.09']\n",
      "   ['0.23']\n",
      "   [0.15]\n",
      "   [0.12]]]\n",
      "\n",
      "\n",
      " [[['1.19']\n",
      "   ['0.99']\n",
      "   ['0.80']\n",
      "   [2.18]\n",
      "   [4.25]]\n",
      "\n",
      "  [['1.20']\n",
      "   ['0.99']\n",
      "   ['0.80']\n",
      "   [2.18]\n",
      "   [4.25]]\n",
      "\n",
      "  [['1.20']\n",
      "   ['0.99']\n",
      "   ['0.80']\n",
      "   [2.18]\n",
      "   [4.25]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [['0.29']\n",
      "   ['0.04']\n",
      "   ['-0.85']\n",
      "   [-0.3]\n",
      "   [0.43]]\n",
      "\n",
      "  [['0.29']\n",
      "   ['0.03']\n",
      "   ['-0.85']\n",
      "   [-0.29]\n",
      "   [0.43]]\n",
      "\n",
      "  [['0.29']\n",
      "   ['0.03']\n",
      "   ['-0.85']\n",
      "   [-0.29]\n",
      "   [0.44]]]\n",
      "\n",
      "\n",
      " [[['3046793.92']\n",
      "   ['131889.93']\n",
      "   ['56944.03']\n",
      "   [18120.35]\n",
      "   [16102.58]]\n",
      "\n",
      "  [['3042372.06']\n",
      "   ['131961.08']\n",
      "   ['56910.23']\n",
      "   [18124.14]\n",
      "   [16107.67]]\n",
      "\n",
      "  [['3033528.38']\n",
      "   ['131977.08']\n",
      "   ['56890.00']\n",
      "   [18132.06]\n",
      "   [16104.88]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [['579966.02']\n",
      "   ['15931.86']\n",
      "   ['30528.20']\n",
      "   [3025.56]\n",
      "   [918.36]]\n",
      "\n",
      "  [['583615.42']\n",
      "   ['15987.33']\n",
      "   ['30442.13']\n",
      "   [3026.82]\n",
      "   [918.42]]\n",
      "\n",
      "  [['587724.42']\n",
      "   ['16042.39']\n",
      "   ['30363.64']\n",
      "   [3027.99]\n",
      "   [918.73]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[['5563101.99']\n",
      "   ['478969.09']\n",
      "   ['53215.48']\n",
      "   [3867.72]\n",
      "   [665.92]]\n",
      "\n",
      "  [['165067.21']\n",
      "   ['15375.97']\n",
      "   ['4832.04']\n",
      "   [1497.11]\n",
      "   [1989.22]]\n",
      "\n",
      "  [['1325513.47']\n",
      "   ['133829.03']\n",
      "   ['9951.17']\n",
      "   [4403.79]\n",
      "   [1357.01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [['623507.14']\n",
      "   ['20274.07']\n",
      "   ['7448.50']\n",
      "   [6017.67]\n",
      "   [2907.7]]\n",
      "\n",
      "  [['908681.93']\n",
      "   ['23522.97']\n",
      "   ['6456.97']\n",
      "   [6119.0]\n",
      "   [2305.02]]\n",
      "\n",
      "  [['1655202.97']\n",
      "   ['22712.74']\n",
      "   ['5765.14']\n",
      "   [4162.39]\n",
      "   [1611.46]]]\n",
      "\n",
      "\n",
      " [[['-0.14']\n",
      "   ['0.11']\n",
      "   ['-0.12']\n",
      "   [-0.19]\n",
      "   [-0.08]]\n",
      "\n",
      "  [['-0.15']\n",
      "   ['0.11']\n",
      "   ['-0.12']\n",
      "   [-0.19]\n",
      "   [-0.08]]\n",
      "\n",
      "  [['-0.15']\n",
      "   ['0.11']\n",
      "   ['-0.12']\n",
      "   [-0.19]\n",
      "   [-0.08]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [['-0.01']\n",
      "   ['-0.23']\n",
      "   ['-0.39']\n",
      "   [-0.4]\n",
      "   [-0.1]]\n",
      "\n",
      "  [['-0.01']\n",
      "   ['-0.23']\n",
      "   ['-0.39']\n",
      "   [-0.4]\n",
      "   [-0.1]]\n",
      "\n",
      "  [['-0.01']\n",
      "   ['-0.23']\n",
      "   ['-0.38']\n",
      "   [-0.4]\n",
      "   [-0.1]]]\n",
      "\n",
      "\n",
      " [[['0.63']\n",
      "   ['-0.01']\n",
      "   ['-0.07']\n",
      "   [-0.82]\n",
      "   [-0.19]]\n",
      "\n",
      "  [['0.58']\n",
      "   ['-1.81']\n",
      "   ['-0.40']\n",
      "   [0.19]\n",
      "   [-0.7]]\n",
      "\n",
      "  [['-0.30']\n",
      "   ['-0.15']\n",
      "   ['-0.10']\n",
      "   [0.13]\n",
      "   [-0.55]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [['-0.14']\n",
      "   ['0.05']\n",
      "   ['-0.01']\n",
      "   [0.36]\n",
      "   [-0.34]]\n",
      "\n",
      "  [['-0.40']\n",
      "   ['-1.18']\n",
      "   ['-0.32']\n",
      "   [-0.57]\n",
      "   [-1.06]]\n",
      "\n",
      "  [['1.52']\n",
      "   ['-0.10']\n",
      "   ['-0.54']\n",
      "   [-0.82]\n",
      "   [-0.9]]]]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:21.748199Z",
     "start_time": "2020-07-25T00:58:21.712244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 20:58:21.721873 4560561600 deprecation.py:323] From /anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "batch_size = 15\n",
    "dataset = tf.data.Dataset.from_generator(tf_data_generator,args= [files, batch_size],output_types = (tf.float32, tf.float32),\n",
    "                                                output_shapes = ((None,4000,5,1),(None,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:24.930781Z",
     "start_time": "2020-07-25T00:58:24.927568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((None, 4000, 5, 1), (None,)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:29.126759Z",
     "start_time": "2020-07-25T00:58:27.188818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n",
      "(15, 4000, 5, 1) (15,)\n",
      "tf.Tensor([2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.], shape=(15,), dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for data, labels in dataset:\n",
    "    print(data.shape, labels.shape)\n",
    "    print(labels)\n",
    "    print()\n",
    "    num = num + 1\n",
    "    if num > 7: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T21:02:53.604188Z",
     "start_time": "2020-07-24T21:02:53.597478Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T21:05:05.147877Z",
     "start_time": "2020-07-24T21:05:05.142476Z"
    }
   },
   "outputs": [],
   "source": [
    "fault_folders = [\"0\", \"1\", \"2\"]\n",
    "for folder_name in fault_folders:\n",
    "    os.mkdir(os.path.join(\"./ExtractedFeatures/csv\", folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T21:08:46.308127Z",
     "start_time": "2020-07-24T21:08:46.268900Z"
    }
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Destination path './ExtractedFeatures/csv/0/0_780.csv' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-c3abc10ff613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfault_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./ExtractedFeatures/csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fault_folders[j]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Destination path './ExtractedFeatures/csv/0/0_780.csv' already exists"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    pattern = \"^\" + eval(\"file[24:25]\")\n",
    "    for j in range(len(fault_folders)):\n",
    "        if re.match(pattern, fault_folders[j]):\n",
    "            dest = os.path.join(\"./ExtractedFeatures/csv\",eval(\"fault_folders[j]\"))\n",
    "            shutil.move(file, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:41.772770Z",
     "start_time": "2020-07-25T00:58:41.768748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./ExtractedFeatures/csv/0',\n",
       " './ExtractedFeatures/csv/1',\n",
       " './ExtractedFeatures/csv/2']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"./ExtractedFeatures/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:42.543452Z",
     "start_time": "2020-07-25T00:58:42.488155Z"
    }
   },
   "outputs": [],
   "source": [
    "label_0_files = glob.glob(\"./ExtractedFeatures/csv/0/*\")\n",
    "label_1_files = glob.glob(\"./ExtractedFeatures/csv/1/*\")\n",
    "label_2_files = glob.glob(\"./ExtractedFeatures/csv/2/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:43.462455Z",
     "start_time": "2020-07-25T00:58:43.180728Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:43.712962Z",
     "start_time": "2020-07-25T00:58:43.704969Z"
    }
   },
   "outputs": [],
   "source": [
    "label_0_train, label_0_test = train_test_split(label_0_files, test_size = .2, random_state = 5)\n",
    "label_1_train, label_1_test = train_test_split(label_1_files, test_size = .2, random_state = 55)\n",
    "label_2_train, label_2_test = train_test_split(label_2_files, test_size = .2, random_state = 555)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:44.624892Z",
     "start_time": "2020-07-25T00:58:44.616786Z"
    }
   },
   "outputs": [],
   "source": [
    "label_0_train, label_0_val = train_test_split(label_0_train, test_size = .1, random_state = 5)\n",
    "label_1_train, label_1_val = train_test_split(label_1_train, test_size = .1, random_state = 55)\n",
    "label_2_train, label_2_val = train_test_split(label_2_train, test_size = .1, random_state = 555)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:45.267840Z",
     "start_time": "2020-07-25T00:58:45.264054Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file_names = label_0_train + label_1_train + label_2_train \n",
    "validation_file_names = label_0_val + label_1_val + label_2_val \n",
    "test_file_names = label_0_test + label_1_test + label_2_test \n",
    "\n",
    "# Shuffle data (We don't need to shuffle validation and test data)\n",
    "np.random.shuffle(train_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:46.094216Z",
     "start_time": "2020-07-25T00:58:46.089568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_files: 5832\n",
      "Number of validation_files: 648\n",
      "Number of test_files: 1620\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train_files:\" ,len(train_file_names))\n",
    "print(\"Number of validation_files:\" ,len(validation_file_names))\n",
    "print(\"Number of test_files:\" ,len(test_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:47.431790Z",
     "start_time": "2020-07-25T00:58:47.365756Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [train_file_names, batch_size], \n",
    "                                              output_shapes = ((None,4000,5,1),(None,)),\n",
    "                                              output_types = (tf.float32, tf.float32))\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [validation_file_names, batch_size],\n",
    "                                                   output_shapes = ((None,4000,5,1),(None,)),\n",
    "                                                   output_types = (tf.float32, tf.float32))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(tf_data_generator, args = [test_file_names, batch_size],\n",
    "                                             output_shapes = ((None,4000,5,1),(None,)),\n",
    "                                             output_types = (tf.float32, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T22:49:02.434165Z",
     "start_time": "2020-07-24T22:49:02.430115Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:51.417009Z",
     "start_time": "2020-07-25T00:58:51.413121Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:52.361107Z",
     "start_time": "2020-07-25T00:58:52.300544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 3998, 3, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 1999, 1, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 31984)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                511760    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 511,971\n",
      "Trainable params: 511,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(16, (3,3), activation = \"relu\", input_shape = (4000,5,1)),\n",
    "    layers.MaxPool2D(2,2),\n",
    "    #layers.Conv2D(32, 5, activation = \"relu\"),\n",
    "    #layers.MaxPool2D(2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(16, activation = \"relu\"),\n",
    "    layers.Dense(3, activation = \"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:58:59.613058Z",
     "start_time": "2020-07-25T00:58:59.516279Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T00:59:00.753480Z",
     "start_time": "2020-07-25T00:59:00.743572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch =  584\n",
      "validation_steps =  65\n",
      "steps =  162\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = np.int(np.ceil(len(train_file_names)/batch_size))\n",
    "validation_steps = np.int(np.ceil(len(validation_file_names)/batch_size))\n",
    "steps = np.int(np.ceil(len(test_file_names)/batch_size))\n",
    "print(\"steps_per_epoch = \", steps_per_epoch)\n",
    "print(\"validation_steps = \", validation_steps)\n",
    "print(\"steps = \", steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T01:18:16.085487Z",
     "start_time": "2020-07-25T00:59:01.628915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "584/584 [==============================] - 124s 212ms/step - loss: 294653.5270 - accuracy: 0.3326 - val_loss: 352512.4340 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "584/584 [==============================] - 117s 200ms/step - loss: 39315.7713 - accuracy: 0.3230 - val_loss: 310972.3740 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "584/584 [==============================] - 118s 203ms/step - loss: 1.0966 - accuracy: 0.3352 - val_loss: 310970.5238 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "584/584 [==============================] - 116s 198ms/step - loss: 6379.4747 - accuracy: 0.3318 - val_loss: 259011.6973 - val_accuracy: 0.3318\n",
      "Epoch 5/10\n",
      "584/584 [==============================] - 119s 204ms/step - loss: 1.0957 - accuracy: 0.3261 - val_loss: 259011.7600 - val_accuracy: 0.3426\n",
      "Epoch 6/10\n",
      "584/584 [==============================] - 115s 197ms/step - loss: 1.0965 - accuracy: 0.3354 - val_loss: 259011.2920 - val_accuracy: 0.3380\n",
      "Epoch 7/10\n",
      "584/584 [==============================] - 110s 188ms/step - loss: 1.0909 - accuracy: 0.3308 - val_loss: 259011.4447 - val_accuracy: 0.3410\n",
      "Epoch 8/10\n",
      "584/584 [==============================] - 112s 192ms/step - loss: 1.0899 - accuracy: 0.3285 - val_loss: 259011.9493 - val_accuracy: 0.3441\n",
      "Epoch 9/10\n",
      "584/584 [==============================] - 113s 194ms/step - loss: 1.0896 - accuracy: 0.3356 - val_loss: 259012.3571 - val_accuracy: 0.3441\n",
      "Epoch 10/10\n",
      "584/584 [==============================] - 110s 188ms/step - loss: 1.0884 - accuracy: 0.3410 - val_loss: 259010.9564 - val_accuracy: 0.3457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2acc4080>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data = validation_dataset, steps_per_epoch = steps_per_epoch,\n",
    "         validation_steps = validation_steps, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
